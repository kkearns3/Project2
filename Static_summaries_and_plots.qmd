---
title: "Project 2"
format: html
toc: TRUE
---
# Static Summaries and Plots

Libraries and dependencies
```{r}
library(tidyverse)
library(readr)
# library(tidycensus)

source("helpers.R")
```

## Data

- Public Use Microdata Sample (PUMS) data obtained from the American Community Survey (ACS), whose results come from around 1% of the population in the US (User Guide 4)
- This app contains data for North Carolina, 2023
- Zipped file downloaded from FTP site: https://www2.census.gov/programs-surveys/acs/data/pums/2023/1-Year/
- There is a Margin of Error (MOE) associated with PUMS, as PUMS data are produced via sampling from the full ACS, and some values have been anonymized (User Guide 4)
- PWGTP is the weighting variable for the person file that allows making estimates for the entire population based on the PUMS sample
- PWGTP1-PWGTP80 are for calculating margin of error for the PUMS sample
    + Variance formula provided by the User Guide: VAR(x) = (4/80)*sum{(x_r - x)^2}, where the sum is from 1 to 80 for each of the 80 replicate weights, and x_r are the estimates using each replicate weight, and x is the estimate using PWGTP
- Incomes and earnings need to be adjusted for inflation to make them aligned to 2023 dollars. 
    + ADJINC should be divided by 1,000,000 and multiplied by the PUMS variable
    + Housing unit variables: FINCP, HINCP
    + Person file variables: INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, WAGP
- Housing Dollar Inflation Factor
    + ADJHSG should be divided by 1,000,000 and multiplied by the PUMS variable
    + Housing unit variables: ONP, ELEP, FULP, GASP, GRNTP, INSP, MHP, MRGP, SMOCP, RNTP, SMP, TAXAMT, VALP, WATP
    + ADJHSG is 1,000,000 for 2023, so technically we don't have to adjust these
- It should be noted that while SERIALNO are unique in Housing data, they are not in the Person file (presumably because many households have more than one person)
    + As such, all metrics are calculated based on individuals (using PWGTP), even if the characteristic chosen is measured at the housing level
    + Therefore, certain metrics may be overestimated if the numerical data point is measured at the household level (e.g. property value), and a significant number of SERIALNO have more than one member in the household

Read in csv files from zipped directory - read in as character, use helper function to assign appropriate column types and values

**Person files**

- Note: Dropping "RT" from both data sets because this is "row type", and will be different in the two files. I don't want it to interfere with a join. Even though we only have to join on SERIALNO, doing so creates a lot of duplicated fields (DIVISION, ST, etc.). It is less error-prone to just join on all the matching fields than to drop all the duplicated fields from one data set.
```{r}
person_data <- unzip("data/csv_pnc.zip", files = c("psam_p37.csv")) |> 
  read_csv(col_types = cols(.default = "c"),
           show_col_types = FALSE) |>
  select(-RT)

# number of households with more than one member reported on survey
person_data |> group_by(SERIALNO) |> summarize(count = n()) |> filter(count > 1) |>
  summarize(multi_person_households = n(), total_individuals = sum(count))
```

**Housing files**

```{r}
housing_data <- unzip("data/csv_hnc.zip", files = c("psam_h37.csv")) |> 
  read_csv(col_types = cols(.default = "c"),
           show_col_types = FALSE) |>
  select(-RT)
```

**Combine data**

```{r}
census <- housing_data |>
  inner_join(person_data)

census
```


Verify the data is correct by comparing to the ACS "PUMS Estimates for User Verification" (saved to the data folder, "pums_estimates_23.csv")

```{r}
pums_estimates <- read_csv("data/pums_estimates_23.csv") |>
  filter(STATE == 37) #North Carolina

```


## Data dictionary

- Reference for values for all of the variables in the PUMS data
- Blank values in the data dictionary are represented by a series of b's, with the number of b's determined by the length of the field. These need to be changed to actual blank values for use with the PUMS data
- csv does not have headers
- the first column in the csv has two possible values, NAME or VAL
    + NAME rows have 5 columns, with column 2 being the variable name and column 5 being the descriptive label for that variable
    + VAL rows have 7 columns, and links a set of min and max values (columns 5 and 6) with a meaningful descriptive value (column 7)
- Data dictionary will be read in as one file, and then split into two separate tibbles to have the names and values separate

```{r}
# read in the entire data dictionary (select distinct due to duplicated rows)
data_dictionary <- read.csv("data/PUMS_Data_Dictionary_2023.csv",
                            header = FALSE,
                            col.names = c("row_type", "variable_name", "data_type", "length",
                                          "value", "value_max", "data_label"),
                            na.strings = c("NA", "")) |>
  as_tibble() |>
  distinct()

#### Don't need the following couple sets of code if using both person and housing files
# create a list of variables that is in the NC census data (there are fewer variables in the actual PUMS data than there is in the data dictionary)
# var_list <- tibble(variable_name = names(census))
# 
# # keep all variables in data_dictionary that have a match in the PUMS (var_list)
# data_dictionary <- data_dictionary |>
#   semi_join(var_list, by = "variable_name")

# data dictionary for names
data_dictionary_names <- data_dictionary |>
  filter(row_type == "NAME") |>
  select(variable_name, value)

# data dictionary for values
data_dictionary_values <- data_dictionary |>
  filter(row_type == "VAL") |>
  select(variable_name, data_type, length, value, value_max, data_label)

# filter on the missing values....trying to decide what to do with them...
data_dictionary_values |>
  filter(value == strrep(rep("b", length(data_dictionary_values$value)), 
                         length))

data_dictionary_names
```

```{r}
data_dictionary_values
```


apply the helper function to each column. 

**When everything is working, change the census_clean to just census**

```{r}
#census_clean <- census

for(c in colnames(census)) {
  census[c] <- clean_census_columns(census[c])
}

```

```{r}
census |>
  group_by(HHLDRHISP) |>
  summarise(households = sum(WGTP)) |>
  arrange(desc(households))
```

## Useful Additional Columns

```{r}
# Adding age groups
census <- census |>
  mutate(Age_Group = case_when(
    AGEP < 5 ~ "Age 0-4",
    AGEP < 10 ~ "Age 5-9",
    AGEP < 15 ~ "Age 10-14",
    AGEP < 20 ~ "Age 15-19",
    AGEP < 25 ~ "Age 20-24",
    AGEP < 35 ~ "Age 25-34",
    AGEP < 45 ~ "Age 35-44",
    AGEP < 55 ~ "Age 45-54",
    AGEP < 60 ~ "Age 55-59",
    AGEP < 65 ~ "Age 60-64",
    AGEP < 75 ~ "Age 65-74",
    AGEP < 85 ~ "Age 75-84",
    AGEP >= 85 ~ "Age 85 and over")) 
```

## Margin of Error Calculations

Need to create a helper function for the variance, standard deviation, and 90% confidence level margin of error

- First, work through an example

```{r}

# main estimate (one-way contingency table on Sex)
totals <- census |> 
  group_by(SEX) |> 
  summarize(estimate = sum(PWGTP))

# same estimate on each of the 80 replicate weights
totals_r <- census |> 
  group_by(SEX) |>
  summarize(across(PWGTP1:PWGTP80, sum))

# calculate the squared differences between the estimate and the replicated estimates
 squared_diffs <- totals |>
  inner_join(totals_r) |>
  mutate(across(PWGTP1:PWGTP80, 
                \(x) (estimate - x)^2,
                .names = "sqdiff_{.col}")) 

# take the sum, multiply by 4/80 to create vector of variances
variance <- squared_diffs |>
  select(sqdiff_PWGTP1:sqdiff_PWGTP80) |>
  apply(MARGIN = 1, FUN = \(x) sum(x)*(4/80))

# add the variance vector as a column on the totals
totals$variance <- variance

totals <- totals |>
  mutate(variance = variance,
         std_err = round(sqrt(variance)),
         margin_of_error = round(sqrt(variance) * 1.645))

totals
```

Check accuracy

```{r}
pums_estimates |>
  filter(CHARACTERISTIC %in% c("Total population",
                               "Total males (SEX=1)",
                               "Total females (SEX=2)"))
```

Can this work for the custom mean function, which also need to take in PWGTP?

```{r}
# HINCP: Household income (past 12 months, use ADJINC to adjust HINCP to constant dollars)

# main estimate (one-way contingency table on Sex)
totals <- census |>
  group_by(SEX) |>
  drop_na(SEX, HINCP) |>
  summarize(estimate = census_mean(HINCP, PWGTP))

# same estimate on each of the 80 replicate weights
totals_r <- census |> 
  group_by(SEX) |>
  drop_na(SEX, HINCP) |>
  summarize(across(PWGTP1:PWGTP80, \(x) census_mean(HINCP, x)))


# calculate the squared differences between the estimate and the replicated estimates
 squared_diffs <- totals |>
  inner_join(totals_r) |>
  mutate(across(PWGTP1:PWGTP80, 
                \(x) (estimate - x)^2,
                .names = "sqdiff_{.col}")) 

# take the sum, multiply by 4/80 to create vector of variances
variance <- squared_diffs |>
  select(sqdiff_PWGTP1:sqdiff_PWGTP80) |>
  apply(MARGIN = 1, FUN = \(x) sum(x)*(4/80))

# add the variance vector as a column on the totals
totals$variance <- variance

totals <- totals |>
  mutate(variance = variance,
         std_err = round(sqrt(variance)),
         margin_of_error = round(sqrt(variance) * 1.645))

totals
```
It works! Hooray!!! Need to put the above into a function

### One-way contingency table

```{r}
census |>
  group_by(HHLDRHISP) |>
  filter(!is.na(HHLDRHISP)) |>
  summarize(individuals = sum(PWGTP)) |>
  arrange(desc(individuals))
```


### Two-way contingency tables

Access to the Internet by SNAP status

```{r}
census |>
  group_by(ACCESSINET, FS) |>
  filter(!is.na(ACCESSINET) & !is.na(FS)) |>
  summarize(individuals = sum(PWGTP)) |>
  pivot_wider(names_from = FS, values_from = individuals)

# FS = "Yearly food stamp/Supplemental Nutrition Assistance Program (SNAP) recipiency"

```


### Numerical summaries

Basic summary

```{r}
# INSP: Fire/hazard/flood insurance (yearly amount, use ADJHSG to adjust INSP to constant dollars)

# main stats
summary_main <- census |>
  group_by(YRBLT) |>
  drop_na(YRBLT, INSP) |>
  summarize(mean = census_mean(INSP, PWGTP),
            median = census_median(INSP, PWGTP))

# error summary
summary_error <- census |>
  group_by(YRBLT) |>
  drop_na(YRBLT, INSP) |>
  do(census_error(., all_of("INSP")))

# final summary - above two should end up with the same grouping variable values in the same order, but going to do a full join just in case
num_summary <- summary_main |>
  full_join(summary_error)

num_summary

```


## Plots

### Map of North Carolina

- Basic plot, not covered in class
- Used tigris package to retrieve PUMAS boundaries
- read tidyverse documentation to get the basics of the plot using census data "https://walker-data.com/census-r/mapping-census-data-with-r.html"
- referenced "https://www.paparkerstat.com/post/plotting-pumas-in-r/" for help in getting the plot to work

```{r}
# get geometric info for the PUMAs
pumas <- tigris::pumas(state = "NC", 
                       year = "2023", 
                       progress_bar = FALSE)

# aggregate census data
census_aggregate <- census |>
  select(PUMA, PWGTP) |>
  group_by(PUMA) |>
  summarize(total = sum(PWGTP))

# join to census data
census_map <- pumas |>
  left_join(census_aggregate, join_by(PUMACE20 == PUMA))

# plot data
ggplot(census_map) +
  geom_sf(aes(fill = total))

```

### Bar plot

```{r}
# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "FS") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "ACCESSINET") |>
  select(value)

g <- ggplot(data = census |> drop_na(FS, ACCESSINET), 
            aes(x = FS, weight = PWGTP, fill = ACCESSINET))

# layers (remove the legend from the base layer, keep the legend for the fill)
g + geom_bar() +
  ggtitle(paste0("Individuals with ", legend_label, "\n", " by ", x_label)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = x_label) +
  scale_fill_discrete(legend_label) +
  facet_wrap(vars(SEX))

```


### Scatter plot

```{r}
# VEH: Vehicles available (capacity of 1 ton or less)
# VALP: Property Value
# AGEP: Age

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "YRBLT") |>
  select(value)

# take sample of the data (1000 points) - note: referenced HW7 app.R file for help with setting up the sample size correctly 
### NOTE: this will need to be pulled from the subset data, not the full data when that is ready.
sample_size <- sample(1:nrow(census), 
                      size = 1000,
                      replace = TRUE,
                      prob = census$PWGTP/sum(census$PWGTP))

# sample for plotting (##NOTE: pull from the census subset, not the full data)
census_sample <- census[sample_size, ]

# base object with global assignments
g <- ggplot(data = census_sample |> drop_na(AGEP, VALP, VEH), 
            aes(x = AGEP, y = VALP, color = VEH, weight = PWGTP))

g + geom_point() 

```


### Kernel density plot

```{r}

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "YRBLT") |>
  select(value)

# base object with global assignments
g <- ggplot(data = census |> drop_na(INSP, YRBLT), aes(x = INSP, weight = PWGTP))

# density plot
g + geom_density(aes(fill = YRBLT), kernel = "gaussian", alpha = 0.4) +
  ggtitle("Test Title") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = x_label, y = "Individuals") +
  scale_fill_discrete(legend_label)

```

### Violin plot

```{r}
census_data <- census |>
  group_by(YRBLT) |>
  drop_na(YRBLT, INSP) |>
  #filter(!is.na(YRBLT) & !is.na(INSP)) |>
  summarize(individuals = sum(PWGTP),
            total = sum(INSP))

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "YRBLT") |>
  select(value)

# base object with global assignments
g <- ggplot(data = census |> drop_na(INSP, YRBLT), aes(y = INSP, weight = PWGTP))

g + geom_violin(aes(x = YRBLT, fill = YRBLT)) +
  ggtitle("Test Title") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = x_label, y = "Individuals") +
  scale_fill_discrete(legend_label)

```

### Heatmap

```{r}
# VEH: Vehicles available (capacity of 1 ton or less)
# VALP: Property Value
# AGEP: Age

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "YRBLT") |>
  select(value)

# take sample of the data (1000 points) - note: referenced HW7 app.R file for help with setting up the sample size correctly 
### NOTE: this will need to be pulled from the subset data, not the full data when that is ready.
sample_size <- sample(1:nrow(census), 
                      size = 1000,
                      replace = TRUE,
                      prob = census$PWGTP/sum(census$PWGTP))

# sample for plotting (##NOTE: pull from the census subset, not the full data)
census_sample <- census[sample_size, ]

# base object with global assignments
g <- ggplot(data = census_sample |> drop_na(Age_Group, VALP, VEH), 
            aes(x = Age_Group, y = VEH, fill = VALP, weight = PWGTP))

g + geom_tile()


```

