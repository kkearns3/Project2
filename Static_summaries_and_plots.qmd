---
title: "Project 2"
format: html
toc: TRUE
---
# Static Summaries and Plots

Libraries and dependencies
```{r}
# "C:/Users/katni/Desktop/Katy Rose/Work & School/NCSU/ST 558 Data Science for Statisticians/Projects/Project2"

library(tidyverse)
library(readr)
# library(tidycensus)

source("helpers.R")
```

## Data

- Public Use Microdata Sample (PUMS) data obtained from the American Community Survey (ACS), whose results come from around 1% of the population in the US (User Guide 4)
- This app contains data for North Carolina, 2023
- Zipped file downloaded from FTP site: https://www2.census.gov/programs-surveys/acs/data/pums/2023/1-Year/
- There is a Margin of Error (MOE) associated with PUMS, as PUMS data are produced via sampling from the full ACS, and some values have been anonymized (User Guide 4)
- PWGTP is the weighting variable for the person file that allows making estimates for the entire population based on the PUMS sample
- PWGTP1-PWGTP80 are for calculating margin of error for the PUMS sample
    + Variance formula provided by the User Guide: VAR(x) = (4/80)*sum{(x_r - x)^2}, where the sum is from 1 to 80 for each of the 80 replicate weights, and x_r are the estimates using each replicate weight, and x is the estimate using PWGTP
- Incomes and earnings need to be adjusted for inflation to make them aligned to 2023 dollars. 
    + ADJINC should be divided by 1,000,000 and multiplied by the PUMS variable
    + Housing unit variables: FINCP, HINCP
    + Person file variables: INTP, OIP, PAP, PERNP, PINCP, RETP, SEMP, SSIP, SSP, WAGP
- Housing Dollar Inflation Factor
    + ADJHSG should be divided by 1,000,000 and multiplied by the PUMS variable
    + Housing unit variables: ONP, ELEP, FULP, GASP, GRNTP, INSP, MHP, MRGP, SMOCP, RNTP, SMP, TAXAMT, VALP, WATP
    + ADJHSG is 1,000,000 for 2023, so technically we don't have to adjust these
- It should be noted that while SERIALNO are unique in Housing data, they are not in the Person file (presumably because many households have more than one person)
    + As such, all metrics are calculated based on individuals (using PWGTP), even if the characteristic chosen is measured at the housing level
    + Therefore, certain metrics may be overestimated if the numerical data point is measured at the household level (e.g. property value), and a significant number of SERIALNO have more than one member in the household

Read in csv files from zipped directory - read in as character, use helper function to assign appropriate column types and values

**Person files**

- Note: Dropping "RT" from both data sets because this is "row type", and will be different in the two files. I don't want it to interfere with a join. Even though we only have to join on SERIALNO, doing so creates a lot of duplicated fields (DIVISION, ST, etc.). It is less error-prone to just join on all the matching fields than to drop all the duplicated fields from one data set.
```{r}
person_data <- unzip("data/csv_pnc.zip", files = c("psam_p37.csv")) |> 
  read_csv(col_types = cols(.default = "c"),
           show_col_types = FALSE) |>
  select(-RT)

# number of households with more than one member reported on survey
person_data |> group_by(SERIALNO) |> summarize(count = n()) |> filter(count > 1) |>
  summarize(multi_person_households = n(), total_individuals = sum(count))
```

**Housing files**

```{r}
housing_data <- unzip("data/csv_hnc.zip", files = c("psam_h37.csv")) |> 
  read_csv(col_types = cols(.default = "c"),
           show_col_types = FALSE) |>
  select(-RT)

housing_data |> group_by(SERIALNO) |> summarize(count = n()) |> filter(count > 1)
```

**Combine data**

```{r}
census <- housing_data |>
  inner_join(person_data)

census
```


Verify the data is correct by comparing to the ACS "PUMS Estimates for User Verification" (saved to the data folder, "pums_estimates_23.csv")

```{r}
pums_estimates <- read_csv("data/pums_estimates_23.csv") |>
  filter(STATE == 37) #North Carolina

```


## Data dictionary

- Reference for values for all of the variables in the PUMS data
- Blank values in the data dictionary are represented by a series of b's, with the number of b's determined by the length of the field. These need to be changed to actual blank values for use with the PUMS data
- csv does not have headers
- the first column in the csv has two possible values, NAME or VAL
    + NAME rows have 5 columns, with column 2 being the variable name and column 5 being the descriptive label for that variable
    + VAL rows have 7 columns, and links a set of min and max values (columns 5 and 6) with a meaningful descriptive value (column 7)
- Data dictionary will be read in as one file, and then split into two separate tibbles to have the names and values separate

```{r}
# read in the entire data dictionary (select distinct due to duplicated rows)
data_dictionary <- read.csv("data/PUMS_Data_Dictionary_2023.csv",
                            header = FALSE,
                            col.names = c("row_type", "variable_name", "data_type", "length",
                                          "value", "value_max", "data_label"),
                            na.strings = c("NA", "")) |>
  as_tibble() |>
  distinct()

#### Don't need the following couple sets of code if using both person and housing files
# create a list of variables that is in the NC census data (there are fewer variables in the actual PUMS data than there is in the data dictionary)
# var_list <- tibble(variable_name = names(census))
# 
# # keep all variables in data_dictionary that have a match in the PUMS (var_list)
# data_dictionary <- data_dictionary |>
#   semi_join(var_list, by = "variable_name")

# data dictionary for names
data_dictionary_names <- data_dictionary |>
  filter(row_type == "NAME") |>
  select(variable_name, value)

# data dictionary for values
data_dictionary_values <- data_dictionary |>
  filter(row_type == "VAL") |>
  select(variable_name, data_type, length, value, value_max, data_label)

# filter on the missing values....trying to decide what to do with them...
data_dictionary_values |>
  filter(value == strrep(rep("b", length(data_dictionary_values$value)), 
                         length))

data_dictionary_names
```

```{r}
data_dictionary_values
```


Clean up data dictionaries so really long descriptions and value labels are renamed

```{r}

###   rename really long descriptions

data_dictionary_names[data_dictionary_names$variable_name == "FS", "value"] <-
  "Food Stamp/SNAP Recipiency"

data_dictionary_names[data_dictionary_names$variable_name == "INSP", "value"] <-
  "Fire/hazard/flood insurance (Amt/yr)"

data_dictionary_names[data_dictionary_names$variable_name == "MRGP", "value"] <-
  "First mortgage payment (Amt/mo)"

data_dictionary_names[data_dictionary_names$variable_name == "HINCP", "value"] <-
  "Household income (past 12 mo)"

data_dictionary_names[data_dictionary_names$variable_name == "OCPIP", "value"] <-
  "Monthly owner costs (% HH income, past 12 mo)"

data_dictionary_names[data_dictionary_names$variable_name == "PINCP", "value"] <-
  "Total person's income"

### rename really long values 

data_dictionary_values[data_dictionary_values$variable_name == "TEN" & 
                         data_dictionary_values$value == "1", "data_label"] <-
  "Owned, mortgage/loan"

data_dictionary_values[data_dictionary_values$variable_name == "TEN" & 
                         data_dictionary_values$value == "4", "data_label"] <-
  "Occupied, no rent"

data_dictionary_values[data_dictionary_values$variable_name == "TEN", ]
```


apply the helper function to each column. 

```{r}
#census_clean <- census

for(c in colnames(census)) {
  census[c] <- clean_census_columns(census[c])
}

```


## Add Grouped Variables

(Run after cleaning census data)

```{r}
# Adding age groups (for testing)
# census <- census |>
#   mutate(Age_Group = case_when(
#     AGEP < 5 ~ "Age 0-4",
#     AGEP < 10 ~ "Age 5-9",
#     AGEP < 15 ~ "Age 10-14",
#     AGEP < 20 ~ "Age 15-19",
#     AGEP < 25 ~ "Age 20-24",
#     AGEP < 35 ~ "Age 25-34",
#     AGEP < 45 ~ "Age 35-44",
#     AGEP < 55 ~ "Age 45-54",
#     AGEP < 60 ~ "Age 55-59",
#     AGEP < 65 ~ "Age 60-64",
#     AGEP < 75 ~ "Age 65-74",
#     AGEP < 85 ~ "Age 75-84",
#     AGEP >= 85 ~ "Age 85 and over"))

# Age groups (for production)
census <- census |>
  mutate(AGEP_Grp = factor(case_when(
                                 AGEP < 18 ~ "Age 0-17",
                                 AGEP < 30 ~ "Age 18-29",
                                 AGEP < 45 ~ "Age 30-44",
                                 AGEP < 60 ~ "Age 45-59",
                                 AGEP < 75 ~ "Age 60-74",
                                 AGEP < 90 ~ "Age 75-89",
                                 AGEP >= 90 ~ "Age 90+"),
                           levels = c("Age 0-17", "Age 18-29", "Age 30-44", "Age 45-59",                                       "Age 60-74", "Age 75-89", "Age 90+")),
         BLD_Grp = factor(case_when(
                               grepl("One-family", BLD)  ~ "Single-family house",
                               grepl("apartments", tolower(BLD)) ~ "Apartments",
                               .default = BLD),
                          levels = c("Mobile home or trailer", "Single-family house",
                                     "Apartments", "Boat, RV, van, etc.")),
         YRBLT_Grp = as.factor(case_when(
                                 YRBLT %in% c("1939 or earlier", "1940 to 1949") ~ 
                                   "1949 or earlier",
                                 YRBLT %in% c("1950 to 1959", "1960 to 1969") ~ 
                                   "1950 to 1969",
                                 YRBLT %in% c("1970 to 1979", "1980 to 1989") ~
                                   "1970 to 1989",
                                 YRBLT %in% c("1990 to 1999", "2000 to 2009") ~
                                   "1990 to 2009",
                                 YRBLT %in% c("2010 to 2019", "2020", "2021", 
                                              "2022", "2023") ~ 
                                   "2010 or later"))
         )

# factorize


# check values
census |>
  group_by(YRBLT_Grp, YRBLT) |>
  summarize(count = sum(PWGTP))

census |>
  group_by(BLD_Grp, BLD) |>
  summarize(count = sum(PWGTP))

census |>
  group_by(AGEP_Grp) |>
  summarize(min = min(AGEP),
            max = max(AGEP), 
            count = sum(PWGTP))
```

Add custom variables to names and values dictionaries

```{r}
#   - add custom variables with descriptions (e.g. AGEP_Grp)
data_dictionary_names <- data_dictionary_names |>
  add_row(variable_name = c("AGEP_Grp", "BLD_Grp", "YRBLT_Grp"),
          value = c("Age Group", "Building Unit Type", "Year Built"))

# add AGEP_Group values to the values dictionary 
new_values <- census |>
  select(AGEP_Grp) |>
  distinct() |>
  mutate(variable_name = "AGEP_Grp", data_type = "C", length = 50,
         value = as.character(AGEP_Grp), 
         value_max = as.character(AGEP_Grp), 
         data_label = as.character(AGEP_Grp)) |>
  select(-AGEP_Grp)

data_dictionary_values <- data_dictionary_values |>
  bind_rows(new_values)

# add BLD_Grp values to the values dictionary 
new_values <- census |>
  select(BLD_Grp) |>
  distinct() |>
  filter(!is.na(BLD_Grp)) |>
  mutate(variable_name = "BLD_Grp", data_type = "C", length = 50,
         value = as.character(BLD_Grp), 
         value_max = as.character(BLD_Grp), 
         data_label = as.character(BLD_Grp)) |>
  select(-BLD_Grp)

data_dictionary_values <- data_dictionary_values |>
  bind_rows(new_values)


# add YRBLT_Grp values to the values dictionary 
new_values <- census |>
  select(YRBLT_Grp) |>
  distinct() |>
  filter(!is.na(YRBLT_Grp)) |>
  mutate(variable_name = "YRBLT_Grp", data_type = "C", length = 50,
         value = as.character(YRBLT_Grp), 
         value_max = as.character(YRBLT_Grp), 
         data_label = as.character(YRBLT_Grp)) |>
  select(-YRBLT_Grp)

data_dictionary_values <- data_dictionary_values |>
  bind_rows(new_values)

# confirm no duplicates
data_dictionary_values |>
  group_by(variable_name, value) |>
  summarize(count = n()) |>
  filter(count > 1)

# check values
data_dictionary_values |>
  filter(variable_name %in% c("BLD_Grp", "AGEP_Grp", "YRBLT_Grp"))

```

## Save .rds files for App Access

Update: 11/6/2024 - app is using too much memory. Going to try to reduce file size for the census data and see if that reduces the memory usage

```{r}
need_cols <- c("AGEP_Grp", "BLD_Grp", "TEN", "VEH", "YRBLT_Grp", "HHLDRHISP",
               "HHL", "MV", "SEX", "NATIVITY", "ACCESSINET", "FS", "TYPEHUGQ",
               "INSP", "AGEP", "MRGP", "VALP", "HINCP", "OCPIP", "PINCP",
               "BLD", "YRBLT", "PWGTP", "PUMA")

census_new <- census |>
  select(any_of(need_cols), PWGTP1:PWGTP10)
```


Save required tables as .rds files in the active directory

```{r}
saveRDS(data_dictionary_names, "dd_names.rds")
saveRDS(data_dictionary_values, "dd_values.rds")
saveRDS(census_new, "census.rds")

# also do this with pumas to try to avoid a call to tigris
# NC map PUMAS geographic table 
pumas <- tigris::pumas(state = "NC", 
                       year = "2023", 
                       progress_bar = FALSE)

saveRDS(pumas, "pumas.rds")
```


## Margin of Error Calculations

Need to create a helper function for the variance, standard deviation, and 90% confidence level margin of error

- First, work through an example

```{r}

# main estimate (one-way contingency table on Sex)
totals <- census |> 
  group_by(SEX) |> 
  summarize(estimate = sum(PWGTP))

# same estimate on each of the 80 replicate weights
totals_r <- census |> 
  group_by(SEX) |>
  summarize(across(PWGTP1:PWGTP80, sum))

# calculate the squared differences between the estimate and the replicated estimates
 squared_diffs <- totals |>
  inner_join(totals_r) |>
  mutate(across(PWGTP1:PWGTP80, 
                \(x) (estimate - x)^2,
                .names = "sqdiff_{.col}")) 

# take the sum, multiply by 4/80 to create vector of variances
variance <- squared_diffs |>
  select(sqdiff_PWGTP1:sqdiff_PWGTP80) |>
  apply(MARGIN = 1, FUN = \(x) sum(x)*(4/80))

# add the variance vector as a column on the totals
totals$variance <- variance

totals <- totals |>
  mutate(variance = variance,
         std_err = round(sqrt(variance)),
         margin_of_error = round(sqrt(variance) * 1.645))

totals
```

Check accuracy

```{r}
pums_estimates |>
  filter(CHARACTERISTIC %in% c("Total population",
                               "Total males (SEX=1)",
                               "Total females (SEX=2)"))
```

Can this work for the custom mean function, which also need to take in PWGTP?

```{r}
# HINCP: Household income (past 12 months, use ADJINC to adjust HINCP to constant dollars)

# main estimate (one-way contingency table on Sex)
totals <- census |>
  group_by(SEX) |>
  drop_na(SEX, HINCP) |>
  summarize(estimate = census_mean(HINCP, PWGTP))

# same estimate on each of the 80 replicate weights
totals_r <- census |> 
  group_by(SEX) |>
  drop_na(SEX, HINCP) |>
  summarize(across(PWGTP1:PWGTP80, \(x) census_mean(HINCP, x)))


# calculate the squared differences between the estimate and the replicated estimates
 squared_diffs <- totals |>
  inner_join(totals_r) |>
  mutate(across(PWGTP1:PWGTP80, 
                \(x) (estimate - x)^2,
                .names = "sqdiff_{.col}")) 

# take the sum, multiply by 4/80 to create vector of variances
variance <- squared_diffs |>
  select(sqdiff_PWGTP1:sqdiff_PWGTP80) |>
  apply(MARGIN = 1, FUN = \(x) sum(x)*(4/80))

# add the variance vector as a column on the totals
totals$variance <- variance

totals <- totals |>
  mutate(variance = variance,
         std_err = round(sqrt(variance)),
         margin_of_error = round(sqrt(variance) * 1.645))

totals
```
It works! Hooray!!! Need to put the above into a function

## Summaries

### One-way contingency table

```{r}
census |>
  group_by(HHLDRHISP) |>
  filter(!is.na(HHLDRHISP)) |>
  summarize(individuals = sum(PWGTP)) |>
  arrange(desc(individuals))
```


### Two-way contingency tables

Access to the Internet by SNAP status

```{r}
census |>
  group_by(ACCESSINET, FS) |>
  filter(!is.na(ACCESSINET) & !is.na(FS)) |>
  summarize(individuals = sum(PWGTP)) |>
  pivot_wider(names_from = FS, values_from = individuals)

# FS = "Yearly food stamp/Supplemental Nutrition Assistance Program (SNAP) recipiency"

```


### Numerical summaries

Basic summary

```{r}
# INSP: Fire/hazard/flood insurance (yearly amount, use ADJHSG to adjust INSP to constant dollars)

# main stats
summary_main <- census |>
  group_by(YRBLT_Grp) |>
  drop_na(YRBLT_Grp, INSP) |>
  summarize(mean = census_mean(INSP, PWGTP),
            median = census_median(INSP, PWGTP))

# error summary
summary_error <- census |>
  group_by(YRBLT_Grp) |>
  drop_na(YRBLT_Grp, INSP) |>
  do(census_error(., all_of("INSP")))

# final summary - above two should end up with the same grouping variable values in the same order, but going to do a full join just in case
num_summary <- summary_main |>
  full_join(summary_error)

num_summary

```


## Plots

### Map of North Carolina

- Basic plot, not covered in class
- Used tigris package to retrieve PUMAS boundaries
- read tidyverse documentation to get the basics of the plot using census data "https://walker-data.com/census-r/mapping-census-data-with-r.html"
- referenced "https://www.paparkerstat.com/post/plotting-pumas-in-r/" for help in getting the plot to work

```{r}
# get geometric info for the PUMAs
pumas <- tigris::pumas(state = "NC", 
                       year = "2023", 
                       progress_bar = FALSE)

# aggregate census data
census_aggregate <- census |>
  group_by(PUMA) |>
  summarize(median = census_median(INSP, PWGTP))

# join to census data
census_map <- pumas |>
  left_join(census_aggregate, join_by(PUMACE20 == PUMA))

x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)


# plot data
ggplot(census_map) +
  geom_sf(aes(fill = median)) +
  ggtitle(paste0("Median value of ", x_label, "\n", " by PUMA")) +
  theme(plot.title = element_text(hjust = 0.5)) 

```

### Bar plot

```{r}
# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "FS") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "ACCESSINET") |>
  select(value)

g <- ggplot(data = census |> drop_na(FS, ACCESSINET), 
            aes(x = FS, weight = PWGTP, fill = ACCESSINET))

# layers (remove the legend from the base layer, keep the legend for the fill)
g + geom_bar() +
  ggtitle(paste0("Individuals with ", legend_label, "\n", " by ", x_label)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = x_label) +
  scale_fill_discrete(legend_label) +
  facet_wrap(vars(SEX))

```


### Scatter plot

```{r}
# VEH: Vehicles available (capacity of 1 ton or less)
# VALP: Property Value
# AGEP: Age

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)

y_label <- data_dictionary_names |>
  filter(variable_name == "VALP") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "VEH") |>
  select(value)

# take sample of the data (1000 points) - note: referenced HW7 app.R file for help with setting up the sample size correctly 
### NOTE: this will need to be pulled from the subset data, not the full data when that is ready.
sample_size <- sample(1:nrow(census), 
                      size = 1000,
                      replace = TRUE,
                      prob = census$PWGTP/sum(census$PWGTP))

# sample for plotting (##NOTE: pull from the census subset, not the full data)
census_sample <- census[sample_size, ]

# base object with global assignments
g <- ggplot(data = census_sample |> drop_na(AGEP, VALP, VEH), 
            aes(x = AGEP, y = VALP, color = VEH, weight = PWGTP))

g + geom_point() +
  labs(x = x_label, y = y_label, caption = paste0("VEH", ": ", legend_label)) +
  theme(plot.caption = element_text(face="italic"))

```


### Kernel density plot

```{r}

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "YRBLT") |>
  select(value)

# base object with global assignments
g <- ggplot(data = census |> drop_na(INSP, YRBLT), aes(x = INSP, weight = PWGTP))

# density plot
g + geom_density(aes(fill = YRBLT), kernel = "gaussian", alpha = 0.4) +
  ggtitle("Test Title") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = x_label, y = "Individuals") +
  scale_fill_discrete(legend_label)

```

### Violin plot

```{r}
census_data <- census |>
  group_by(YRBLT) |>
  drop_na(YRBLT, INSP) |>
  #filter(!is.na(YRBLT) & !is.na(INSP)) |>
  summarize(individuals = sum(PWGTP),
            total = sum(INSP))

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "INSP") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "YRBLT") |>
  select(value)

# base object with global assignments
g <- ggplot(data = census |> drop_na(INSP, YRBLT), aes(y = INSP, weight = PWGTP))

g + geom_violin(aes(x = YRBLT, fill = YRBLT)) +
  ggtitle("Test Title") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = x_label, y = "Individuals") +
  scale_fill_discrete(legend_label)

```

### Heatmap

```{r}
# VEH: Vehicles available (capacity of 1 ton or less)
# VALP: Property Value
# AGEP: Age

# values for labels
x_label <- data_dictionary_names |>
  filter(variable_name == "AGEP_Grp") |>
  select(value)

y_label <- data_dictionary_names |>
  filter(variable_name == "VEH") |>
  select(value)

legend_label <- data_dictionary_names |>
  filter(variable_name == "VALP") |>
  select(value)

# take sample of the data (1000 points) - note: referenced HW7 app.R file for help with setting up the sample size correctly 
### NOTE: this will need to be pulled from the subset data, not the full data when that is ready.
sample_size <- sample(1:nrow(census), 
                      size = 1000,
                      replace = TRUE,
                      prob = census$PWGTP/sum(census$PWGTP))

# sample for plotting (##NOTE: pull from the census subset, not the full data)
census_sample <- census[sample_size, ]

# base object with global assignments
g <- ggplot(data = census_sample |> drop_na(AGEP_Grp, VALP, VEH), 
            aes(x = AGEP_Grp, y = VEH, fill = VALP, weight = PWGTP))

g + geom_tile() +
  ggtitle(paste0("Heatmap of ", x_label, "\n", " by ", y_label)) +
      theme(plot.title = element_text(hjust = 0.5)) +
      labs(x = x_label, y = y_label) +
      guides(fill = guide_colorbar(title = legend_label))


```

